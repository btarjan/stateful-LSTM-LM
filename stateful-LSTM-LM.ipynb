{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "from os import path\n",
    "from math import exp\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Input\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.callbacks import History\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## text preprocessing for cross sentence training\n",
    "def cross_sentence_preproc(text):\n",
    "    return text.replace('\\n',' <eos> ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tokenize textfile and save as pickle file\n",
    "def tokenize_txt_cross_sentence(textfile):\n",
    "    ## open the file as read only\n",
    "    file = open(textfile, 'r', encoding='UTF-8')\n",
    "    ## read all text\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    print('\\nSample of original txt:\\n\\n', text[:300])\n",
    "    \n",
    "    ## run text preprocessing\n",
    "    text_proc = cross_sentence_preproc(text)\n",
    "    print('\\nSample of processed txt:\\n\\n', text_proc[:300])\n",
    "    print('\\nTotal tokens in text: %d' % len(text_proc.split()))\n",
    "    print('Unique tokens in text: %d' % len(set(text_proc.split())))\n",
    "    \n",
    "    ## fit tokenizer\n",
    "    tokenizer = Tokenizer(filters='', lower=False)\n",
    "    tokenizer.fit_on_texts([text_proc])\n",
    "    ## saving tokenizer\n",
    "    with open('tokenizer/cs-{0}.pickle'.format(path.splitext(path.basename(textfile))[0]), 'wb') as handle:\n",
    "        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Converting text to lm training tensor for cross sentence LSTM\n",
    "def txt_to_tensor_cross_sent_LSTM(textfile, tokenizer, batch_size=20, seq_length=35):\n",
    "    \n",
    "    ## open the file as read only\n",
    "    file = open(textfile, 'r', encoding='UTF-8')\n",
    "    \n",
    "    ## read all text\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    print('\\nSample of original txt:\\n\\n', text[:300])\n",
    "    \n",
    "    ## run text preprocessing\n",
    "    text_proc = cross_sentence_preproc(text)\n",
    "    print('\\nSample of processed txt:\\n\\n', text_proc[:300])\n",
    "    print('\\nTotal tokens in text: %d' % len(text_proc.split()))\n",
    "    print('Unique tokens in text: %d' % len(set(text_proc.split())))\n",
    "    \n",
    "    ## load tokenizer\n",
    "    with open(tokenizer, 'rb') as handle:\n",
    "        tokenizer = pickle.load(handle)\n",
    "\n",
    "    ## vocabulary size\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    print('Vocab size: %d' % vocab_size)\n",
    "\n",
    "    ## coding text\n",
    "    text_coded = tokenizer.texts_to_sequences([text_proc])\n",
    "    text_coded_len = len(text_coded[0])\n",
    "    print('Coded text length:', text_coded_len)\n",
    "    \n",
    "    ## pad according to current batch size and seq length (stateful training requirement!)\n",
    "    padding_length = batch_size * seq_length * ( (text_coded_len // (batch_size * seq_length)) + 1 )\n",
    "    input_array = pad_sequences(text_coded, padding='post', maxlen=padding_length)[0,:]\n",
    "    print('Padded input array shape:', input_array.shape)\n",
    "    \n",
    "    ## creat target array from input array\n",
    "    target_array = input_array.copy()\n",
    "    target_array[0:-1] = input_array[1:]\n",
    "    target_array[-1] = input_array[0]\n",
    "    \n",
    "    ## reshaping input and target array to fit stateful training\n",
    "    ## reshaping according to batch_size\n",
    "    input_array = input_array.reshape((batch_size, -1))\n",
    "    target_array = target_array.reshape((batch_size, -1))\n",
    "    ## creating list of batches (link: ...)\n",
    "    x_batches = np.split(input_array, input_array.shape[1] // seq_length, axis=1)\n",
    "    y_batches = np.split(target_array, target_array.shape[1] // seq_length, axis=1)\n",
    "    assert len(x_batches) == len(y_batches)\n",
    "    \n",
    "    ## concatenting list of batches (fit instead of fit generator)\n",
    "    X = np.concatenate(x_batches)\n",
    "    y = np.concatenate(y_batches)\n",
    "    ## additional rank for y array (Keras requirement)\n",
    "    y = y.reshape(y.shape[0], y.shape[1], 1)\n",
    "\n",
    "    print('Input tensor shape:' , X.shape)\n",
    "    print('Target tensor shape:' , y.shape)\n",
    "\n",
    "    return X, y, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluation of cross sentence LSTMs\n",
    "def cross_sentence_LSTM_eval(model_path, testfile, tokenizer, batch_size=20, seq_length=35):\n",
    "    model = load_model(model_path)\n",
    "    X_test, y_test, _ = txt_to_tensor_cross_sent_LSTM(testfile, tokenizer, batch_size, seq_length)\n",
    "    test_loss = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
    "    print(testfile)\n",
    "    print('Loss: %f\\nPerplexity: %f\\n\\n' % (test_loss, exp(test_loss)))\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## stateful 2-layer LSTM for cross sentence modeling\n",
    "def LSTM_stateful_model(vocab_size, embedding_dim, batch_size, seq_length, dropout_rate, LSTM_hidden_size,\n",
    "                        embedding_matrix = 'None'):\n",
    "    model = Sequential()\n",
    "    if embedding_matrix == 'None':\n",
    "        model.add(Embedding(vocab_size, embedding_dim, batch_input_shape=(batch_size, seq_length), mask_zero=True))\n",
    "    else:\n",
    "        model.add(Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], batch_input_shape=(batch_size, seq_length),\n",
    "                  trainable=True, mask_zero=True))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(LSTM(LSTM_hidden_size, recurrent_activation='sigmoid', return_sequences=True, stateful=True))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(LSTM(LSTM_hidden_size, recurrent_activation='sigmoid', return_sequences=True, stateful=True))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(TimeDistributed(Dense(vocab_size, activation='softmax')))\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## text generation with cross sentence LSTM\n",
    "def cross_sentence_LSTM_generate(model_path, tokenizer, seed_text='', num_words=100, temperature=1.0,\n",
    "                                 batch_size=20, seq_length=35, random_seed=None):\n",
    "    \n",
    "    ## seed numpy random\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    ## load model\n",
    "    model = load_model(model_path)\n",
    "    ## load tokenizer\n",
    "    with open(tokenizer, 'rb') as handle:\n",
    "        tokenizer = pickle.load(handle)\n",
    "    \n",
    "    ## creat id to word mapping dictionary\n",
    "    word_to_id = tokenizer.word_index\n",
    "    id_to_word = {}\n",
    "    for c, i in word_to_id.items():\n",
    "        id_to_word[i] = c\n",
    "    ## add key 0 to dictionary if does not exist\n",
    "    if 0 not in id_to_word:\n",
    "        id_to_word[0] = '<mask>'\n",
    "    \n",
    "    ## coding seed text\n",
    "    sentence = [word_to_id[word] for word in seed_text.split()]\n",
    "\n",
    "    for i in range(num_words):\n",
    "        ## úgy pad-eljük, hogy seq_length hosszú legyen\n",
    "        sentence_padded = pad_sequences([sentence], maxlen=seq_length)\n",
    "        ## seq_length*batch_size hosszúra padd-eljük a sorokat, így a későbbi sorokban csupa nulla lesz\n",
    "        sentence_padded_postzero = pad_sequences(sentence_padded, maxlen=seq_length*batch_size, padding='post')\n",
    "        ## úgy rendezzük, hogy minden batch-ben csak az első sor legyen értékes, többi nulla\n",
    "        sentence_array = np.reshape(sentence_padded_postzero, (-1, seq_length))\n",
    "        ## predikció (az első sor utolsó értékét vizsgáljuk)\n",
    "        preds = model.predict(sentence_array, batch_size=batch_size)[0,-1]\n",
    "        ## mintavételezzük az eloszlást\n",
    "        next_index = sample_pred_simple(preds)\n",
    "        ## a mintavételezett kódot átfordítjuk szóra\n",
    "        next_word = id_to_word[next_index]\n",
    "        ## a következő bemenő szó a mostani kimenő szó lesz (stateful háló megjegyzi az előzményt!)\n",
    "        sentence = [next_index]\n",
    "        ## kiírjuk a következő szót\n",
    "        sys.stdout.write((next_word if next_word != '<eos>' else '\\n') + ' ')\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## training function for cross sentence LSTM\n",
    "def cross_sentence_LSTM_train(train_txt, valid_txt, eval_txt, tokenizer, optimizer, callbacks, pretrained_embedding = 'None',\n",
    "                              batch_size = 20, epochs = 100, dropout_rate = 0.5, embedding_dim = 650, LSTM_hidden_size = 650,\n",
    "                              seq_length = 35):\n",
    "\n",
    "    ## extract train txt filename\n",
    "    train_txt_fn = path.splitext(path.basename(train_txt))[0]\n",
    "    ## Creating tensors\n",
    "    X, y, vocab_size = txt_to_tensor_cross_sent_LSTM(train_txt, tokenizer, batch_size, seq_length)\n",
    "    X_valid, y_valid, _ = txt_to_tensor_cross_sent_LSTM(valid_txt, tokenizer, batch_size, seq_length)\n",
    "    ## define model\n",
    "    if pretrained_embedding == 'None':\n",
    "        model = LSTM_stateful_model(vocab_size, embedding_dim, batch_size, seq_length, dropout_rate, LSTM_hidden_size)\n",
    "    else:\n",
    "        ## creating pretrained embedding matrix\n",
    "        embedding_matrix = create_embedding_matrix(pretrained_embedding, tokenizer, embedding_dim)\n",
    "        model = LSTM_stateful_model(vocab_size, embedding_dim, batch_size, seq_length, dropout_rate, LSTM_hidden_size,\n",
    "                                    embedding_matrix = embedding_matrix)\n",
    "    ## compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer)\n",
    "    ## extract pretrained embedding filename\n",
    "    pretrained_embedding_fn = path.splitext(path.basename(pretrained_embedding))[0]\n",
    "    ## compile model name\n",
    "    model_fn = 'model/cs-LSTM_{0}_BS-{1}_EMB-{2}.h5'.format(train_txt_fn, batch_size, pretrained_embedding_fn)\n",
    "    ## append model saving to callbacks\n",
    "    callbacks.append(ModelCheckpoint(model_fn, monitor='val_loss', save_best_only=True))\n",
    "    ## fit model\n",
    "    model.fit(X, y, epochs=epochs, callbacks=callbacks, validation_data=(X_valid, y_valid), batch_size=batch_size, shuffle=False)\n",
    "    ## evaluation\n",
    "    cross_sentence_LSTM_eval(model_fn, valid_txt, tokenizer, batch_size, seq_length)\n",
    "    cross_sentence_LSTM_eval(model_fn, eval_txt, tokenizer, batch_size, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## global variables\n",
    "batch_size = 20\n",
    "epochs = 100\n",
    "seq_length = 35\n",
    "LSTM_hidden_size = 650\n",
    "embedding_dim = 650\n",
    "dropout_rate = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## training and validation data\n",
    "train_txt='data/ptb/ptb_train.txt'\n",
    "valid_txt='data/ptb/ptb_valid.txt'\n",
    "eval_txt='data/ptb/ptb_test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of original txt:\n",
      "\n",
      "  aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia memotec mlx nahb punts rake regatta rubens sim snack-food ssangyong swapo wachter \n",
      " pierre <unk> N years old will join the board as a nonexecutive director nov. N \n",
      " mr. <unk> is chairman of <unk> n.v. the d\n",
      "\n",
      "Sample of processed txt:\n",
      "\n",
      "  aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia memotec mlx nahb punts rake regatta rubens sim snack-food ssangyong swapo wachter  <eos>  pierre <unk> N years old will join the board as a nonexecutive director nov. N  <eos>  mr. <unk> is chairman of <unk\n",
      "\n",
      "Total tokens in text: 929589\n",
      "Unique tokens in text: 10000\n"
     ]
    }
   ],
   "source": [
    "## tokenize train text\n",
    "tokenize_txt_cross_sentence(train_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tokenizer\n",
    "tokenizer='tokenizer/cs-ptb_train.pickle'\n",
    "\n",
    "## optimizer\n",
    "optimizer = optimizers.SGD(lr=1.0, momentum=0.9)\n",
    "\n",
    "## Create callback for early stopping on validation loss\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=3), \n",
    "             ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=0, min_lr=0.001, verbose=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of original txt:\n",
      "\n",
      "  aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia memotec mlx nahb punts rake regatta rubens sim snack-food ssangyong swapo wachter \n",
      " pierre <unk> N years old will join the board as a nonexecutive director nov. N \n",
      " mr. <unk> is chairman of <unk> n.v. the d\n",
      "\n",
      "Sample of processed txt:\n",
      "\n",
      "  aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia memotec mlx nahb punts rake regatta rubens sim snack-food ssangyong swapo wachter  <eos>  pierre <unk> N years old will join the board as a nonexecutive director nov. N  <eos>  mr. <unk> is chairman of <unk\n",
      "\n",
      "Total tokens in text: 929589\n",
      "Unique tokens in text: 10000\n",
      "Vocab size: 10001\n",
      "Coded text length: 929589\n",
      "Padded input array shape: (929600,)\n",
      "Input tensor shape: (26560, 35)\n",
      "Target tensor shape: (26560, 35, 1)\n",
      "\n",
      "Sample of original txt:\n",
      "\n",
      "  consumers may want to move their telephones a little closer to the tv set \n",
      " <unk> <unk> watching abc 's monday night football can now vote during <unk> for the greatest play in N years from among four or five <unk> <unk> \n",
      " two weeks ago viewers of several nbc <unk> consumer segments started calling\n",
      "\n",
      "Sample of processed txt:\n",
      "\n",
      "  consumers may want to move their telephones a little closer to the tv set  <eos>  <unk> <unk> watching abc 's monday night football can now vote during <unk> for the greatest play in N years from among four or five <unk> <unk>  <eos>  two weeks ago viewers of several nbc <unk> consumer segments sta\n",
      "\n",
      "Total tokens in text: 73760\n",
      "Unique tokens in text: 6022\n",
      "Vocab size: 10001\n",
      "Coded text length: 73760\n",
      "Padded input array shape: (74200,)\n",
      "Input tensor shape: (2120, 35)\n",
      "Target tensor shape: (2120, 35, 1)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (20, 35, 650)             6500650   \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (20, 35, 650)             0         \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (20, 35, 650)             3382600   \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (20, 35, 650)             0         \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (20, 35, 650)             3382600   \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (20, 35, 650)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (20, 35, 10001)           6510651   \n",
      "=================================================================\n",
      "Total params: 19,776,501\n",
      "Trainable params: 19,776,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 26560 samples, validate on 2120 samples\n",
      "Epoch 1/100\n",
      " 9880/26560 [==========>...................] - ETA: 1:48 - loss: 7.8709"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-8cbd0f7bcdc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m cross_sentence_LSTM_train(train_txt, valid_txt, eval_txt, tokenizer, optimizer, callbacks, batch_size=batch_size, \n\u001b[1;32m      2\u001b[0m                           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropout_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                           LSTM_hidden_size=LSTM_hidden_size, seq_length=seq_length)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-30-ee16ba9aed66>\u001b[0m in \u001b[0;36mcross_sentence_LSTM_train\u001b[0;34m(train_txt, valid_txt, eval_txt, tokenizer, optimizer, callbacks, pretrained_embedding, batch_size, epochs, dropout_rate, embedding_dim, LSTM_hidden_size, seq_length)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m## fit model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;31m## evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mcross_sentence_LSTM_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_txt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1400\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cross_sentence_LSTM_train(train_txt, valid_txt, eval_txt, tokenizer, optimizer, callbacks, batch_size=batch_size, \n",
    "                          epochs=epochs, dropout_rate=dropout_rate, embedding_dim=embedding_dim, \n",
    "                          LSTM_hidden_size=LSTM_hidden_size, seq_length=seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
