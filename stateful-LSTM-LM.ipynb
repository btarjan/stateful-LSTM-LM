{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "from os import path\n",
    "from math import exp\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Input\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.callbacks import History\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## text preprocessing for cross sentence training\n",
    "def cross_sentence_preproc(text):\n",
    "    return text.replace('\\n',' <eos> ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tokenize textfile and save as pickle file\n",
    "def tokenize_txt_cross_sentence(textfile):\n",
    "    ## open the file as read only\n",
    "    file = open(textfile, 'r', encoding='UTF-8')\n",
    "    ## read all text\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    print('\\nSample of original txt:\\n\\n', text[:300])\n",
    "    \n",
    "    ## run text preprocessing\n",
    "    text_proc = cross_sentence_preproc(text)\n",
    "    print('\\nSample of processed txt:\\n\\n', text_proc[:300])\n",
    "    print('\\nTotal tokens in text: %d' % len(text_proc.split()))\n",
    "    print('Unique tokens in text: %d' % len(set(text_proc.split())))\n",
    "    \n",
    "    ## fit tokenizer\n",
    "    tokenizer = Tokenizer(filters='', lower=False)\n",
    "    tokenizer.fit_on_texts([text_proc])\n",
    "    ## saving tokenizer\n",
    "    with open('tokenizer/cs-{0}.pickle'.format(path.splitext(path.basename(textfile))[0]), 'wb') as handle:\n",
    "        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Converting text to lm training tensor for cross sentence LSTM\n",
    "def txt_to_tensor_cross_sent_LSTM(textfile, tokenizer, batch_size=20, seq_length=35):\n",
    "    \n",
    "    ## open the file as read only\n",
    "    file = open(textfile, 'r', encoding='UTF-8')\n",
    "    \n",
    "    ## read all text\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    print('\\nSample of original txt:\\n\\n', text[:300])\n",
    "    \n",
    "    ## run text preprocessing\n",
    "    text_proc = cross_sentence_preproc(text)\n",
    "    print('\\nSample of processed txt:\\n\\n', text_proc[:300])\n",
    "    print('\\nTotal tokens in text: %d' % len(text_proc.split()))\n",
    "    print('Unique tokens in text: %d' % len(set(text_proc.split())))\n",
    "    \n",
    "    ## load tokenizer\n",
    "    with open(tokenizer, 'rb') as handle:\n",
    "        tokenizer = pickle.load(handle)\n",
    "\n",
    "    ## vocabulary size\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    print('Vocab size: %d' % vocab_size)\n",
    "\n",
    "    ## coding text\n",
    "    text_coded = tokenizer.texts_to_sequences([text_proc])\n",
    "    text_coded_len = len(text_coded[0])\n",
    "    print('Coded text length:', text_coded_len)\n",
    "    \n",
    "    ## pad according to current batch size and seq length (stateful training requirement!)\n",
    "    padding_length = batch_size * seq_length * ( (text_coded_len // (batch_size * seq_length)) + 1 )\n",
    "    input_array = pad_sequences(text_coded, padding='post', maxlen=padding_length)[0,:]\n",
    "    print('Padded input array shape:', input_array.shape)\n",
    "    \n",
    "    ## creat target array from input array\n",
    "    target_array = input_array.copy()\n",
    "    target_array[0:-1] = input_array[1:]\n",
    "    target_array[-1] = input_array[0]\n",
    "    \n",
    "    ## reshaping input and target array to fit stateful training\n",
    "    ## reshaping according to batch_size\n",
    "    input_array = input_array.reshape((batch_size, -1))\n",
    "    target_array = target_array.reshape((batch_size, -1))\n",
    "    ## creating list of batches (link: ...)\n",
    "    x_batches = np.split(input_array, input_array.shape[1] // seq_length, axis=1)\n",
    "    y_batches = np.split(target_array, target_array.shape[1] // seq_length, axis=1)\n",
    "    assert len(x_batches) == len(y_batches)\n",
    "    \n",
    "    ## concatenting list of batches (fit instead of fit generator)\n",
    "    X = np.concatenate(x_batches)\n",
    "    y = np.concatenate(y_batches)\n",
    "    ## additional rank for y array (Keras requirement)\n",
    "    y = y.reshape(y.shape[0], y.shape[1], 1)\n",
    "\n",
    "    print('Input tensor shape:' , X.shape)\n",
    "    print('Target tensor shape:' , y.shape)\n",
    "\n",
    "    return X, y, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluation of cross sentence LSTMs\n",
    "def cross_sentence_LSTM_eval(model_path, testfile, tokenizer, batch_size=20, seq_length=35):\n",
    "    model = load_model(model_path)\n",
    "    X_test, y_test, _ = txt_to_tensor_cross_sent_LSTM(testfile, tokenizer, batch_size, seq_length)\n",
    "    test_loss = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
    "    print(testfile)\n",
    "    print('Loss: %f\\nPerplexity: %f\\n\\n' % (test_loss, exp(test_loss)))\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## stateful 2-layer LSTM for cross sentence modeling\n",
    "def LSTM_stateful_model(vocab_size, embedding_dim, batch_size, seq_length, dropout_rate, LSTM_hidden_size,\n",
    "                        embedding_matrix = 'None'):\n",
    "    model = Sequential()\n",
    "    if embedding_matrix == 'None':\n",
    "        model.add(Embedding(vocab_size, embedding_dim, batch_input_shape=(batch_size, seq_length), mask_zero=True))\n",
    "    else:\n",
    "        model.add(Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], batch_input_shape=(batch_size, seq_length),\n",
    "                  trainable=True, mask_zero=True))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(LSTM(LSTM_hidden_size, recurrent_activation='sigmoid', return_sequences=True, stateful=True))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(LSTM(LSTM_hidden_size, recurrent_activation='sigmoid', return_sequences=True, stateful=True))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(TimeDistributed(Dense(vocab_size, activation='softmax')))\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## text generation with cross sentence LSTM\n",
    "def cross_sentence_LSTM_generate(model_path, tokenizer, seed_text='', num_words=100, temperature=1.0,\n",
    "                                 batch_size=20, seq_length=35, random_seed=None):\n",
    "    \n",
    "    ## seed numpy random\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    ## load model\n",
    "    model = load_model(model_path)\n",
    "    ## load tokenizer\n",
    "    with open(tokenizer, 'rb') as handle:\n",
    "        tokenizer = pickle.load(handle)\n",
    "    \n",
    "    ## creat id to word mapping dictionary\n",
    "    word_to_id = tokenizer.word_index\n",
    "    id_to_word = {}\n",
    "    for c, i in word_to_id.items():\n",
    "        id_to_word[i] = c\n",
    "    ## add key 0 to dictionary if does not exist\n",
    "    if 0 not in id_to_word:\n",
    "        id_to_word[0] = '<mask>'\n",
    "    \n",
    "    ## coding seed text\n",
    "    sentence = [word_to_id[word] for word in seed_text.split()]\n",
    "\n",
    "    for i in range(num_words):\n",
    "        ## úgy pad-eljük, hogy seq_length hosszú legyen\n",
    "        sentence_padded = pad_sequences([sentence], maxlen=seq_length)\n",
    "        ## seq_length*batch_size hosszúra padd-eljük a sorokat, így a későbbi sorokban csupa nulla lesz\n",
    "        sentence_padded_postzero = pad_sequences(sentence_padded, maxlen=seq_length*batch_size, padding='post')\n",
    "        ## úgy rendezzük, hogy minden batch-ben csak az első sor legyen értékes, többi nulla\n",
    "        sentence_array = np.reshape(sentence_padded_postzero, (-1, seq_length))\n",
    "        ## predikció (az első sor utolsó értékét vizsgáljuk)\n",
    "        preds = model.predict(sentence_array, batch_size=batch_size)[0,-1]\n",
    "        ## mintavételezzük az eloszlást\n",
    "        next_index = sample_pred_simple(preds)\n",
    "        ## a mintavételezett kódot átfordítjuk szóra\n",
    "        next_word = id_to_word[next_index]\n",
    "        ## a következő bemenő szó a mostani kimenő szó lesz (stateful háló megjegyzi az előzményt!)\n",
    "        sentence = [next_index]\n",
    "        ## kiírjuk a következő szót\n",
    "        sys.stdout.write((next_word if next_word != '<eos>' else '\\n') + ' ')\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## training function for cross sentence LSTM\n",
    "def cross_sentence_LSTM_train(train_txt, valid_txt, eval_txt, tokenizer, optimizer, callbacks, pretrained_embedding = 'None',\n",
    "                              batch_size = 20, epochs = 100, dropout_rate = 0.5, embedding_dim = 650, LSTM_hidden_size = 650,\n",
    "                              seq_length = 35):\n",
    "\n",
    "    ## extract train txt filename\n",
    "    train_txt_fn = path.splitext(path.basename(train_txt))[0]\n",
    "    ## Creating tensors\n",
    "    X, y, vocab_size = txt_to_tensor_cross_sent_LSTM(train_txt, tokenizer, batch_size, seq_length)\n",
    "    X_valid, y_valid, _ = txt_to_tensor_cross_sent_LSTM(valid_txt, tokenizer, batch_size, seq_length)\n",
    "    ## define model\n",
    "    if pretrained_embedding == 'None':\n",
    "        model = LSTM_stateful_model(vocab_size, embedding_dim, batch_size, seq_length, dropout_rate, LSTM_hidden_size)\n",
    "    else:\n",
    "        ## creating pretrained embedding matrix\n",
    "        embedding_matrix = create_embedding_matrix(pretrained_embedding, tokenizer, embedding_dim)\n",
    "        model = LSTM_stateful_model(vocab_size, embedding_dim, batch_size, seq_length, dropout_rate, LSTM_hidden_size,\n",
    "                                    embedding_matrix = embedding_matrix)\n",
    "    ## compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer)\n",
    "    ## extract pretrained embedding filename\n",
    "    pretrained_embedding_fn = path.splitext(path.basename(pretrained_embedding))[0]\n",
    "    ## compile model name\n",
    "    model_fn = 'model/cs-LSTM_{0}_BS-{1}_EMB-{2}.h5'.format(train_txt_fn, batch_size, pretrained_embedding_fn)\n",
    "    ## append model saving to callbacks\n",
    "    callbacks.append(ModelCheckpoint(model_fn, monitor='val_loss', save_best_only=True))\n",
    "    ## fit model\n",
    "    model.fit(X, y, epochs=epochs, callbacks=callbacks, verbose=2, validation_data=(X_valid, y_valid), batch_size=batch_size, shuffle=False)\n",
    "    ## evaluation\n",
    "    cross_sentence_LSTM_eval(model_fn, valid_txt, tokenizer, batch_size, seq_length)\n",
    "    cross_sentence_LSTM_eval(model_fn, eval_txt, tokenizer, batch_size, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## global variables\n",
    "batch_size = 20\n",
    "epochs = 100\n",
    "seq_length = 35\n",
    "LSTM_hidden_size = 650\n",
    "embedding_dim = 650\n",
    "dropout_rate = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## training and validation data\n",
    "train_txt='data/ptb/ptb_train.txt'\n",
    "valid_txt='data/ptb/ptb_valid.txt'\n",
    "eval_txt='data/ptb/ptb_test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tokenizer\n",
    "tokenizer='tokenizer/si-ptb_train.pickle'\n",
    "\n",
    "## optimizer\n",
    "optimizer = optimizers.SGD(lr=1.0, momentum=0.9)\n",
    "\n",
    "## Create callback for early stopping on validation loss\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=3), \n",
    "             ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=0, min_lr=0.001, verbose=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_sentence_LSTM_train(train_txt, valid_txt, eval_txt, tokenizer, optimizer, callbacks, batch_size=batch_size, \n",
    "                          epochs=epochs, dropout_rate=dropout_rate, embedding_dim=embedding_dim, \n",
    "                          LSTM_hidden_size=LSTM_hidden_size, seq_length=seq_length)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
